{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apparent-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifteen-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model createÂ¶\n",
    "#Initialising the CNN\n",
    "#Convolution\n",
    "#Pooling\n",
    "#Flattening\n",
    "#Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cloudy-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modell compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pretty-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "african-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image augmentation & Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ultimate-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16005 images belonging to 3 classes.\n",
      "Found 4023 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: -8394.5215 - accuracy: 0.2424 - val_loss: -43398.0820 - val_accuracy: 0.2520\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 35s 142ms/step - loss: -201673.5156 - accuracy: 0.2501 - val_loss: -452488.5938 - val_accuracy: 0.2525\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: -967616.8125 - accuracy: 0.2518 - val_loss: -1752294.5000 - val_accuracy: 0.2406\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: -2744969.0000 - accuracy: 0.2553 - val_loss: -4144548.2500 - val_accuracy: 0.2445\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: -5346993.0000 - accuracy: 0.2475 - val_loss: -7523727.5000 - val_accuracy: 0.2564\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: -9778204.0000 - accuracy: 0.2465 - val_loss: -12924204.0000 - val_accuracy: 0.2485\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 34s 138ms/step - loss: -15745781.0000 - accuracy: 0.2559 - val_loss: -19537392.0000 - val_accuracy: 0.2421\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: -24404426.0000 - accuracy: 0.2475 - val_loss: -27224280.0000 - val_accuracy: 0.2371\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: -32694968.0000 - accuracy: 0.2473 - val_loss: -38726088.0000 - val_accuracy: 0.2584\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: -44862656.0000 - accuracy: 0.2495 - val_loss: -47551744.0000 - val_accuracy: 0.2614\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: -57624628.0000 - accuracy: 0.2416 - val_loss: -65903080.0000 - val_accuracy: 0.2525\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: -71983912.0000 - accuracy: 0.2516 - val_loss: -80583416.0000 - val_accuracy: 0.2401\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -90477056.0000 - accuracy: 0.2471 - val_loss: -103895496.0000 - val_accuracy: 0.2421\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -101565024.0000 - accuracy: 0.2522 - val_loss: -114999496.0000 - val_accuracy: 0.2505\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -122828328.0000 - accuracy: 0.2524 - val_loss: -132454176.0000 - val_accuracy: 0.2411\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -153628128.0000 - accuracy: 0.2551 - val_loss: -161365408.0000 - val_accuracy: 0.2431\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -170543504.0000 - accuracy: 0.2505 - val_loss: -178217712.0000 - val_accuracy: 0.2520\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -208150160.0000 - accuracy: 0.2530 - val_loss: -213035696.0000 - val_accuracy: 0.2515\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -233538960.0000 - accuracy: 0.2496 - val_loss: -248620400.0000 - val_accuracy: 0.2540\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -260947072.0000 - accuracy: 0.2435 - val_loss: -291382176.0000 - val_accuracy: 0.2465\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -300249696.0000 - accuracy: 0.2519 - val_loss: -308760320.0000 - val_accuracy: 0.2530\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: -337029152.0000 - accuracy: 0.2515 - val_loss: -351927136.0000 - val_accuracy: 0.2431\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: -358791936.0000 - accuracy: 0.2512 - val_loss: -384268416.0000 - val_accuracy: 0.2569\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: -422665120.0000 - accuracy: 0.2528 - val_loss: -439551776.0000 - val_accuracy: 0.2465\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: -448047168.0000 - accuracy: 0.2527 - val_loss: -497683232.0000 - val_accuracy: 0.2436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb4a0658fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##create an object of ImageDataGenerator, for augmenting train set\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "##create another object of ImageDataGenerator, for augmenting test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "##apply image augmentation on train set by resizing all images to 64x64 and creating batches of 32 images.\n",
    "training_set = train_datagen.flow_from_directory('/home/soetcse/Documents/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "##apply image augmentation on test set by resizing all images to 64x64 and creating batches of 32 images.\n",
    "test_set = test_datagen.flow_from_directory('/home/soetcse/Documents/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "\n",
    "###steps_per_epoch: num of data divided by batch size\n",
    "###validation_steps: num of data divided by batch size\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = (8000/32),\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = (2000/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aware-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "former-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img('/home/soetcse/Documents/single_prediction/cat_or_dog_1.jpg', \n",
    "                            target_size = (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "advance-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add channel dimension for image\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "metric-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "##add batch dimension for image\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "junior-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "copyrighted-impact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1, 'training_set': 2}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "stopped-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-radius",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
